# configs/non_reasoning.yaml
model:
  name: "non_reasoning"
  base_model: "Qwen/Qwen2.5-3B"
  max_seq_length: 2048
  lora_r: 32
  lora_alpha: 64
  lora_dropout: 0.051
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  use_cache: false                    # Disable KV cache for training
  device_map: "auto"
  trust_remote_code: true
  attn_implementation: "auto"        # Use "flash_attention_2" if available
  dtype: null 

training:
  output_dir: "./outputs"

  # Training duration control
  use_epochs: true                    # true = epoch-based, false = step-based
  num_train_epochs: 3                 # For epoch-based training
  max_steps: 50                       # For step-based training
  eval_log_frequency: 0.2             # Eval/log at every 20% completion

  # Training parameters
  learning_rate: 0.0002
  batch_size: 32                      # Starting point for auto-detection
  auto_find_batch_size: false          # Enable automatic batch size optimization
  gradient_accumulation_steps: 1
  warmup_steps: 5

  # These will be auto-calculated based on use_epochs flag
  logging_steps: null                 # Auto-calculated (20% intervals)
  save_steps: null                    # Auto-calculated (20% intervals)
  eval_steps: null                    # Auto-calculated (20% intervals)
  load_in_4bit: false
  optim: "adamw_torch"   
  lr_scheduler_type: "linear"           
  weight_decay: 0.01
  max_grad_norm: 1.0 
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  group_by_length: true
  remove_unused_columns: false
  dataloader_drop_last: true
  gradient_checkpointing: false     
  fp16: false
  bf16: true                       
  use_gradient_checkpointing: false
  evaluation_strategy: "steps"
  load_best_model_at_end: true
  save_strategy: "steps"
  save_only_model: true
  save_total_limit: 1  
  metric_for_best_model: "eval_loss"
  greater_is_better: false             
  run_name: null
  seed: 42
  data_seed: 3407
  report_to: "wandb"                    # Enable wandb
  wandb_project: "finetune-my-diary"    # Your project name
  wandb_entity: "akashe"                     # Your username (or null for default)
  wandb_tags: ["non-reasoning", "qwen", "fp16", "diary", "personal-data"]
  run_name: null 
