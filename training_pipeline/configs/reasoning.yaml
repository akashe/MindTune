# configs/reasoning.yaml
model:
  name: "reasoning"
  base_model: "unsloth/llama-2-7b-bnb-4bit"  # Same base for fair comparison
  max_seq_length: 2048
  lora_r: 64  
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  use_cache: false
  device_map: "auto"
  trust_remote_code: true
  attn_implementation: "auto"
  dtype: null

training:
  output_dir: "./outputs"
  max_steps: 10
  learning_rate: 0.0001  # Slightly lower for reasoning training
  batch_size: 16
  gradient_accumulation_steps: 1
  warmup_steps: 1
  logging_steps: 2
  save_steps: 2
  eval_steps: 2
  load_in_4bit: true
  optim: "adamw_torch"
  lr_scheduler_type: "linear"
  weight_decay: 0.05                       # Higher weight decay for generalization
  max_grad_norm: 0.5  
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  group_by_length: true                    # Important for varied reasoning lengths
  remove_unused_columns: false
  dataloader_drop_last: true
  gradient_checkpointing: false            # Keep off for speed
  fp16: null
  bf16: null
  use_gradient_checkpointing: false
  evaluation_strategy: "steps"
  load_best_model_at_end: true
  save_strategy: "steps"
  save_only_model: true
  save_total_limit: 1  
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  run_name: null
  seed: 42
  data_seed: 3407
  report_to: "wandb"                    # Enable wandb
  wandb_project: "finetune-my-diary"    # Your project name
  wandb_entity: "akashe"                    # Your username (or null for default)
  wandb_tags: ["non-reasoning", "diary", "personal-data"]
  run_name: null 