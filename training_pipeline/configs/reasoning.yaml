# configs/reasoning.yaml
model:
  name: "reasoning"
  base_model: "Qwen/Qwen2.5-3B"  # Same base for fair comparison
  max_seq_length: 2048
  lora_r: 64  
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  use_cache: false
  device_map: "auto"
  trust_remote_code: true
  attn_implementation: "auto"
  dtype: null

training:
  output_dir: "./outputs"

  # Training duration control
  use_epochs: true                    # true = epoch-based, false = step-based
  num_train_epochs: 1                 # For epoch-based training
  max_steps: 30                       # For step-based training
  eval_log_frequency: 0.2             # Eval/log at every 20% completion

  # Training parameters
  learning_rate: 0.0001               # Slightly lower for reasoning training
  batch_size: 128                      # Starting point (lower for longer sequences)
  auto_find_batch_size: false          # Enable automatic batch size optimization
  gradient_accumulation_steps: 1
  warmup_steps: 1

  # These will be auto-calculated based on use_epochs flag
  logging_steps: null                 # Auto-calculated (20% intervals)
  save_steps: null                    # Auto-calculated (20% intervals)
  eval_steps: null                    # Auto-calculated (20% intervals)
  load_in_4bit: false
  optim: "adamw_torch"
  lr_scheduler_type: "linear"
  weight_decay: 0.05                       # Higher weight decay for generalization
  max_grad_norm: 0.5  
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  group_by_length: true                    # Important for varied reasoning lengths
  remove_unused_columns: false
  dataloader_drop_last: true
  gradient_checkpointing: false            # Keep off for speed
  fp16: false
  bf16: true
  use_gradient_checkpointing: false
  evaluation_strategy: "steps"
  load_best_model_at_end: true
  save_strategy: "steps"
  save_only_model: true
  save_total_limit: 1  
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  run_name: null
  seed: 42
  data_seed: 3407
  report_to: "wandb"                    # Enable wandb
  wandb_project: "finetune-my-diary"    # Your project name
  wandb_entity: "akashe"                    # Your username (or null for default)
  wandb_tags: ["reasoning", "qwen", "fp16", "diary", "personal-data"]
  run_name: null 